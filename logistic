# --- Step 1: Import necessary libraries ---
import seaborn as sns                          # For statistical visualization
import pandas as pd                            # For data manipulation and DataFrame handling
from sklearn.model_selection import train_test_split # To split data into training and testing sets
from sklearn.linear_model import LogisticRegression  # A simple yet effective classification model
from sklearn.metrics import accuracy_score, classification_report # To evaluate the model's performance
import matplotlib.pyplot as plt                # For creating basic plots

# --- Step 2: Load the Iris Dataset 
df = pd.read_csv('path/to/your/local_iris.csv') 

# --- Step 3: Data Preprocessing and Cleaning (Checking and Imputing Missing Values) ---

# Check for missing values before imputation
print("Missing values per column (Initial check):\n", df.isnull().sum()) 

# Define numeric features to clean
numeric_features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']

# Handle potential missing values (Imputation):
# Fill any NaNs in the numeric feature columns with the column mean.
df[numeric_features] = df[numeric_features].fillna(df[numeric_features].mean()) 

# Check again to confirm NaNs are handled (This should show all zeros)
print("\nMissing values per column (after imputation):\n", df.isnull().sum())

# Separate features (X) and target variable (y)
X = df.drop('species', axis=1)                 # X contains the four measurement columns (features)
y = df['species']                              # y contains the 'species' names (target)

# Split the data into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)      # Use random_state=42 for reproducible results

# --- Step 4: Model Training (Logistic Regression) ---

model = LogisticRegression(max_iter=200)       # Initialize the Logistic Regression model
model.fit(X_train, y_train)                    # Train the model using the training data

# --- Step 5: Predictions ---

y_pred = model.predict(X_test)                 # Generate predictions on the unseen test set

# --- Step 6: Accuracy/Performance Metrics ---

print("\n--- Performance Metrics ---")         # Print a section header

# Calculate the overall accuracy
accuracy = accuracy_score(y_test, y_pred)      # Compare predictions (y_pred) with true test labels (y_test)
print(f"Accuracy Score: {accuracy:.4f}")       # Print the calculated accuracy score

# Print a detailed classification report (Precision, Recall, F1-Score)
print("\nClassification Report:")              # Print header for detailed metrics
print(classification_report(y_test, y_pred))   # Generate and print the report for all classes

# --- Step 7: Basic Graph (Scatter Plot) ---

plt.figure(figsize=(8, 6))                     # Create a new figure for the visualization
# Use seaborn to create a scatter plot of two features, colored by the target variable
sns.scatterplot(x='sepal_length', y='sepal_width', hue='species', data=df, s=70, palette='viridis')
plt.title('Iris Sepal Length vs. Sepal Width by Species') # Set a descriptive title for the plot
plt.xlabel('Sepal Length (cm)')                # Label the x-axis
plt.ylabel('Sepal Width (cm)')                 # Label the y-axis
plt.legend(title='Species')                    # Show the legend to identify colors
plt.grid(True, linestyle=':', alpha=0.7)       # Add a light grid for better readability
plt.show()                                     # Display the resulting plot
